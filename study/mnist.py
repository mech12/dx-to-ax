# ì¼€ë¼ìŠ¤ - í…ì„œ ì„¤ì¹˜ì‹œ ìë™ì„¤ì¹˜( í…ì„œ ë‚´ë¶€ì— í¸ì…) , 
# íŒŒì´í† ì¹˜( LLM ê°œë°œìë“¤ì´ ì‚¬ìš© )  - ë³„ë„ ì„¤ì¹˜
import tensorflow as tf
from tensorflow import keras

# model =>  í•™ìŠµëœ ë°ì´íƒ€ = ì‹ ê²½ë§ = ë„¤íŠ¸ì›  : ë™ì¼í•œ ì˜ë¯¸ë¡œ ì“°ì¸ë‹¤.
# ì‹¬ì¸µì‹ ê²½ë§ :  ì‹ ê²½ë§ ë ˆì´ì–´ë¥¼ ê²¹ê²¹ì´ ìŒ“ì•˜ë‹¤. 
from tensorflow.keras.models import Sequential
# layers.Dense => ê²¹ê²¹íˆ ìŒ“ìœ¼ë©´ í•™ìŠµì´ ì˜ ë˜ëŠ”ë° ë„ˆë¬´ ìŒ“ìœ¼ë©´ ê³¼ëŒ€ì í•©ì´ ë¨. 
# ì ë‹¹íˆ ìŒ“ì•„ì•¼ í•œë‹¤. ë‹µì€ ì•„ë¬´ë„ ëª¨ë¥¸ë‹¤.
from tensorflow.keras.layers import Dense, Flatten

import numpy as np
import matplotlib.pyplot as plt

# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# Keras APIë¥¼ í†µí•´ MNIST ë°ì´í„°ì…‹ ë¡œë“œ
# ë¯¸êµ­ ìš°í¸êµ­ì—ì„œ ëª¨ì€ 7ë§Œê°œ ì†ê¸€ì”¨ ìë£Œ
# ì´ë¯¸ì§€ë¥¼ numpy ë°°ì—´ë¡œ ë°”ê¿”ì•¼ í•œë‹¤.
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

# í›ˆë ¨ì…‹ê³¼ í…ŒìŠ¤íŠ¸ì…‹ì„ ìª¼ê°œì„œ í›ˆë ¨ì…‹ì´ ê³¼ëŒ€ì í•©ë˜ëŠ”ê²ƒì„ ë§‰ëŠ”ë‹¤.
print("\n" + "="*60)
print("MNIST ë°ì´í„°ì…‹ ì •ë³´")
print("="*60)

print("\n[ë°ì´í„° í˜•íƒœ (Shape)]")
print(f"  x_train (í›ˆë ¨ ì´ë¯¸ì§€): {x_train.shape}")
print(f"    â†’ {x_train.shape[0]:,}ê°œ ìƒ˜í”Œ, {x_train.shape[1]}x{x_train.shape[2]} í”½ì…€")
print(f"  y_train (í›ˆë ¨ ë ˆì´ë¸”): {y_train.shape}")
print(f"    â†’ {y_train.shape[0]:,}ê°œ ë ˆì´ë¸”")
print()
print(f"  x_test (í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€): {x_test.shape}")
print(f"    â†’ {x_test.shape[0]:,}ê°œ ìƒ˜í”Œ, {x_test.shape[1]}x{x_test.shape[2]} í”½ì…€")
print(f"  y_test (í…ŒìŠ¤íŠ¸ ë ˆì´ë¸”): {y_test.shape}")
print(f"    â†’ {y_test.shape[0]:,}ê°œ ë ˆì´ë¸”")

print("\n[ì „ì²´ ìš”ì†Œ ê°œìˆ˜ (Size)]")
print(f"  x_train ì „ì²´ í”½ì…€ ìˆ˜: {x_train.size:,} = {x_train.shape[0]:,} Ã— {x_train.shape[1]} Ã— {x_train.shape[2]}")
print(f"  x_test ì „ì²´ í”½ì…€ ìˆ˜: {x_test.size:,} = {x_test.shape[0]:,} Ã— {x_test.shape[1]} Ã— {x_test.shape[2]}")
print(f"  y_train ë ˆì´ë¸” ê°œìˆ˜: {y_train.size:,}")
print(f"  y_test ë ˆì´ë¸” ê°œìˆ˜: {y_test.size:,}")

print("\n[ë°ì´í„° íƒ€ì… ë° ê°’ ë²”ìœ„]")
print(f"  x_train ë°ì´í„° íƒ€ì…: {x_train.dtype}")
print(f"  x_train ê°’ ë²”ìœ„: {x_train.min()} ~ {x_train.max()}")
print(f"  y_train ë°ì´í„° íƒ€ì…: {y_train.dtype}")
print(f"  y_train í´ë˜ìŠ¤: {np.unique(y_train)} (0~9 ìˆ«ì)")

print("\n[ë ˆì´ë¸” ë¶„í¬]")
unique, counts = np.unique(y_train, return_counts=True)
for label, count in zip(unique, counts):
    print(f"  ìˆ«ì {label}: {count:,}ê°œ")

print("="*60 + "\n")

# ì´ë¯¸ì§€ í”½ì…€ ê°’ ì •ê·œí™”: 0.0 ~ 1.0 ë²”ìœ„ë¡œ ë³€í™˜
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0


# 2. MLP ëª¨ë¸ ì •ì˜ (tf.keras.Sequential)
# MLPëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ í‰íƒ„í™”(Flatten)í•˜ëŠ” ê²ƒì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤. 
model = tf.keras.Sequential([
    # ì…ë ¥ í˜•íƒœ(28x28)ë¥¼ ì§€ì •í•˜ê³  1ì°¨ì› ë²¡í„°(784)ë¡œ ë³€í™˜
    tf.keras.layers.Flatten(input_shape=(28, 28)), 
    
    # ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ: 512ê°œì˜ ë…¸ë“œ (Dense Layer)
    tf.keras.layers.Dense(512, activation='relu'),
    
    # ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µ: 256ê°œì˜ ë…¸ë“œ (Dense Layer)
    tf.keras.layers.Dense(256, activation='relu'),
    
    # ì¶œë ¥ì¸µ: 10ê°œì˜ í´ë˜ìŠ¤(0-9)ë¥¼ ìœ„í•œ Softmax í™œì„±í™”
    tf.keras.layers.Dense(10, activation='softmax')
])

# 3. ëª¨ë¸ ì»´íŒŒì¼
model.compile(optimizer='adam',
              # ì†ì‹¤ í•¨ìˆ˜: ì •ìˆ˜í˜• ë ˆì´ë¸”ì„ ìœ„í•œ Sparse Categorical Crossentropy
              loss='sparse_categorical_crossentropy', 
              metrics=['accuracy'])

# ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
print("--- ëª¨ë¸ ìš”ì•½ ---")
model.summary()
print("------------------")

# 4. ëª¨ë¸ í•™ìŠµ (Training)
# history ê°ì²´ì— í•™ìŠµ ê³¼ì •ì˜ ì†ì‹¤ ë° ì •í™•ë„ ê¸°ë¡
history = model.fit(
    x_train, 
    y_train, 
    epochs=10, 
    batch_size=128, 
    validation_split=0.1  # í•™ìŠµ ë°ì´í„°ì˜ 10%ë¥¼ ê²€ì¦ì— ì‚¬ìš©
)

# 5. ëª¨ë¸ í‰ê°€ (Evaluation)
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print(f'\nğŸ‰ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì •í™•ë„: {test_acc:.4f}')